

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>jhun-park</title>
  <subtitle>A minimal, responsive and feature-rich Jekyll theme for technical writing.</subtitle>
  <updated>2024-01-30T12:30:04+09:00</updated>
  <author>
    <name>Jeonghun Park</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator>
  <rights> © 2024 Jeonghun Park </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
    <link href="http://localhost:4000/posts/markdown/" rel="alternate" type="text/html" title="Mamba: Linear-Time Sequence Modeling with Selective State Spaces" />
    <published>2024-01-02T02:47:10+09:00</published>
  
    <updated>2024-01-29T06:35:22+09:00</updated>
  
    <id>http://localhost:4000/posts/markdown/</id>
    <content src="http://localhost:4000/posts/markdown/" />
    <author>
      <name>jhun-park</name>
    </author>

  
    
    <category term="Paper Review" />
    
    <category term="Architectures" />
    
  

  
    <summary>
      





      Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers’ computational inefficiency on lo...
    </summary>
  

  </entry>

</feed>


