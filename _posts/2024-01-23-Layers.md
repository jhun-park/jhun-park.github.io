---
title:  "[Deeplearning] Layer"
comments: true
date:   2024-01-30 02:47:10 +0900
categories: [Deeplearning]
tags: [Deeplearning, Layer]
---

- **Fully Connected Layer (Dense Layer)**

- **Convolution Layers**
    - Conv2D / 1D
    - Conv2DTranspose

- **Time Series layers**
    - RNN
    - LSTM
    - GRU
    - SimpleRNN

- **Reshaping Layers**
    - Reshape
    - Concatenate([**layer1**, **layer2**, **…**])
    - Flatten → 1차원화

- **Regularization and Normalization Layers**
    - Batch Normalization → 미니 배치 별로 평균, 분산 계산 → 정규
        
    - Layer Normalization → 샘플 별로 평균, 분산 계산 → 정규화 → but 레이어의 노드 개수 x 2 만큼 _γ_(스케일)과 _β_(이동) 파라미터에 대해 학습
        
        - Layer Normalization **학습 과정**
            
            Layer Normalization에서 \( \gamma \) (스케일)과 \( \beta \) (이동) 파라미터는 학습 과정에서 최적화됩니다. 이 파라미터들은 전체 데이터셋에 대해 학습되며, 각 샘플에 동일하게 적용됩니다. 즉, 각 샘플 별로 평균과 분산을 계산하여 정규화를 수행한 후, 이 학습 가능한 \( \gamma \)와 \( \beta \) 파라미터를 사용하여 정규화된 출력을 스케일링하고 이동시킵니다.
            
            ### 작동 원리
            
            1. **정규화 단계**: 각 샘플 내에서 평균과 분산을 계산하고, 이를 사용하여 샘플을 정규화합니다. \[ \text{Normalized Output} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \] 여기서 \( x \)는 입력, \( \mu \)는 입력의 평균, \( \sigma^2 \)는 입력의 분산, \( \epsilon \)은 작은 상수입니다.
            2. **스케일링과 이동 단계**: 정규화된 출력에 \( \gamma \)와 \( \beta \)를 적용합니다. \[ \text{Final Output} = \text{Normalized Output} \times \gamma + \beta \] 여기서 \( \gamma \)와 \( \beta \)는 학습 가능한 파라미터입니다.
            
            ### 학습 과정
            
            - 초기에 \( \gamma \)는 주로 1에, \( \beta \)는 0에 가깝게 초기화됩니다.
            - 경사 하강법(Gradient Descent) 또는 그 변형 알고리즘을 사용하여, 손실 함수를 최소화하는 \( \gamma \)와 \( \beta \) 값을 찾습니다.
            - 이 과정은 전체 네트워크를 학습시키는 과정의 일부로, 다른 레이어의 파라미터와 함께 최적화됩니다.
            
            따라서 \( \gamma \)와 \( \beta \)는 데이터셋 전체에 대한 정보를 반영하게 되며, 각 샘플에 동일하게 적용됩니다.
            
    - Dropout → 학습시 랜덤하게 노드를 사용 X
        
    - SpatialDropout2D
        
    - AlphaDropout
        
- **Embedding / Feature Transformation Layers**
    - Embedding
    - Flatten
    - Reshape
    - Permute
    - RepeatVector


- **Activation and Non-linearity Layers**
    - ReLU
    - LeakyReLU
    - PReLU
    - Softmax
    - Sigmoid
    - Tanh

- **Pooling / Down-sampling Layers**
    - MaxPooling2D / 1D
    - AveragePooling2D / 1D
    - GlobalMaxPooling2D / 1D
    - GlobalAveragePooling2D / 1D

- **Utility Layers**
    - Input
    - Dense (Fully Connected)
    - Concatenate
    - Add
    - Multiply
    - Lambda